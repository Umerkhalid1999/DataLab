================================================================================
                AUTOMATED WORKFLOW SYSTEM - ARCHITECTURE
================================================================================

VISUAL PIPELINE FLOW:
---------------------

    [START]
       |
       v
+------------------+
|   Node 1:        |  --> Validates CSV file
|  Data Upload     |  --> Checks rows/columns
|                  |  --> Reports dataset info
+------------------+
       |
       v
+------------------+
|   Node 2:        |  --> Removes duplicate rows
|  Data Cleaning   |  --> Handles missing values (drop/mean/median)
|                  |  --> Saves: dataset_cleaned.csv
+------------------+
       |
       v
+------------------+
|   Node 3:        |  --> Statistical summary (mean, std, min, max)
|  Data Profiling  |  --> Data types analysis
|                  |  --> Missing value report
+------------------+
       |
       v
+------------------+
|   Node 4:        |  --> Encodes categorical variables
|  Feature Eng.    |  --> Scales numeric features (StandardScaler)
|                  |  --> Saves: dataset_engineered.csv
+------------------+
       |
       v
+------------------+
|   Node 5:        |  --> Auto-detects problem type
|  ML Training     |  --> Trains Random Forest model
|                  |  --> Evaluates performance
|                  |  --> Saves: dataset_model.pkl
+------------------+
       |
       v
+------------------+
|   Node 6:        |  --> Exports all results
|  Results Export  |  --> Creates summary JSON
|                  |  --> Saves: dataset_results.json
+------------------+
       |
       v
    [COMPLETE]


SYSTEM ARCHITECTURE:
--------------------

Frontend (automated_workflow.html)
    |
    | HTTP Requests
    v
Backend API (automated_workflow_routes.py)
    |
    +-- /api/pipeline-structure (GET)
    |   Returns: Node definitions and connections
    |
    +-- /api/start-execution (POST)
    |   Creates: Execution ID and initial state
    |
    +-- /api/execute-node/<exec_id>/<node_id> (POST)
    |   Executes: Specific module function
    |   Updates: Execution state
    |
    +-- /api/execution-status/<exec_id> (GET)
    |   Returns: Current execution state
    |
    +-- /api/user-datasets (GET)
        Returns: Available datasets for user


EXECUTION FLOW:
---------------

1. User Action:
   - Selects dataset
   - Configures options
   - Clicks "Start Automated Workflow"

2. Frontend:
   - Calls /api/start-execution
   - Receives execution_id
   - Starts sequential node execution

3. For Each Node:
   - Updates UI to "running" state
   - Calls /api/execute-node/<exec_id>/<node_id>
   - Backend executes module function
   - Backend updates execution state
   - Frontend receives result
   - Updates UI to "completed" or "failed"
   - Moves to next node

4. Completion:
   - All nodes completed
   - Shows results panel
   - Displays metrics and file paths


DATA FLOW:
----------

Original CSV File
    |
    v
[Node 1: Validation]
    |
    v
[Node 2: Cleaning] --> dataset_cleaned.csv
    |
    v
[Node 3: Profiling] --> Statistical Report
    |
    v
[Node 4: Feature Eng.] --> dataset_engineered.csv
    |
    v
[Node 5: ML Training] --> dataset_model.pkl
    |
    v
[Node 6: Export] --> dataset_results.json


MODULE INTEGRATION:
-------------------

Automated Workflow System
    |
    +-- Uses: Data Upload Module (Module 1)
    |   Function: Load and validate CSV
    |
    +-- Uses: Data Cleaning Module (Module 2)
    |   Function: Remove duplicates, handle missing values
    |
    +-- Uses: EDA Module (Module 3)
    |   Function: Statistical profiling
    |
    +-- Uses: Feature Engineering Module (Module 6)
    |   Function: Encode and scale features
    |
    +-- Uses: ML Module
        Function: Train and evaluate models


STATE MANAGEMENT:
-----------------

workflow_executions = {
    'exec_user123_dataset456_20240101120000': {
        'id': 'exec_user123_dataset456_20240101120000',
        'user_id': 'user123',
        'dataset_id': 'dataset456',
        'status': 'running',  # running | completed | failed
        'current_node': 'node3',
        'completed_nodes': ['node1', 'node2'],
        'failed_nodes': [],
        'results': {
            'node1': { 'rows': 1000, 'columns': 10 },
            'node2': { 'rows_removed': 50, 'nulls_removed': 100 }
        },
        'logs': [
            {'timestamp': '...', 'node_id': 'node1', 'status': 'success'},
            {'timestamp': '...', 'node_id': 'node2', 'status': 'success'}
        ],
        'started_at': '2024-01-01T12:00:00',
        'config': {
            'missing_value_strategy': 'mean',
            'scale_features': true
        }
    }
}


UI COMPONENTS:
--------------

1. Control Panel:
   - Dataset selector dropdown
   - Configuration options
   - Start/Stop buttons

2. Progress Bar:
   - Shows 0-100% completion
   - Updates after each node

3. Pipeline Canvas:
   - Visual node representation
   - Color-coded status
   - Animated transitions

4. Logs Console:
   - Real-time log entries
   - Color-coded messages
   - Auto-scroll to bottom

5. Results Panel:
   - Displays node results
   - Shows metrics and stats
   - Provides file paths


ERROR HANDLING:
---------------

Node Execution Error:
    |
    v
1. Node marked as "failed" (RED)
2. Error logged to console
3. Workflow stops at failed node
4. Previous nodes remain completed
5. User can restart workflow


CONFIGURATION OPTIONS:
----------------------

Missing Value Strategy:
- drop: Remove rows with any missing values
- mean: Fill numeric columns with mean
- median: Fill numeric columns with median
- mode: Fill with most frequent value

Scale Features:
- true: Apply StandardScaler to numeric features
- false: Keep original numeric values


PERFORMANCE:
------------

Sequential Execution:
- Ensures data consistency
- Each node waits for previous completion
- No parallel execution (by design)

Typical Execution Time:
- Small dataset (<1000 rows): 10-30 seconds
- Medium dataset (1000-10000 rows): 30-60 seconds
- Large dataset (>10000 rows): 60-120 seconds

Real-time Updates:
- Status updates: Immediate
- Progress bar: After each node
- Logs: Real-time streaming


SECURITY:
---------

Authentication:
- @login_required decorator on all routes
- Session-based user identification

Data Isolation:
- User-specific dataset access
- Execution state per user
- File path validation

Input Validation:
- Dataset ID verification
- Configuration parameter validation
- File existence checks


API ENDPOINTS REFERENCE:
------------------------

GET /automated-workflow/
    Description: Main workflow page
    Returns: HTML page

GET /automated-workflow/api/pipeline-structure
    Description: Get pipeline nodes and edges
    Returns: {
        'success': true,
        'nodes': [...],
        'edges': [...]
    }

POST /automated-workflow/api/start-execution
    Body: {
        'dataset_id': '123',
        'config': {
            'missing_value_strategy': 'mean',
            'scale_features': true
        }
    }
    Returns: {
        'success': true,
        'execution_id': 'exec_...',
        'message': 'Workflow execution started'
    }

POST /automated-workflow/api/execute-node/<execution_id>/<node_id>
    Description: Execute specific node
    Returns: {
        'success': true,
        'node_id': 'node1',
        'result': {...},
        'execution': {...}
    }

GET /automated-workflow/api/execution-status/<execution_id>
    Description: Get current execution status
    Returns: {
        'success': true,
        'execution': {...}
    }

GET /automated-workflow/api/user-datasets
    Description: Get user's available datasets
    Returns: {
        'success': true,
        'datasets': [...]
    }


FUTURE ENHANCEMENTS:
--------------------

Potential additions:
- Parallel execution for independent nodes
- Custom node configuration per execution
- Workflow templates (save/load configurations)
- Export workflow as Python script
- Schedule automated runs (cron-like)
- Email notifications on completion
- Webhook integrations
- Multi-user collaboration
- Version control for workflows
- A/B testing capabilities


DEPLOYMENT NOTES:
-----------------

Development:
- Run: python main.py
- Access: http://localhost:5000/automated-workflow

Production:
- Use WSGI server (Gunicorn, uWSGI)
- Enable HTTPS
- Configure proper logging
- Set up monitoring
- Implement rate limiting
- Add caching layer


TESTING:
--------

Manual Testing:
1. Upload a dataset
2. Navigate to /automated-workflow
3. Select dataset
4. Configure options
5. Click "Start Automated Workflow"
6. Verify each node executes
7. Check results panel

Automated Testing:
- Unit tests for each module function
- Integration tests for API endpoints
- End-to-end tests for complete workflow
- Performance tests for large datasets


TROUBLESHOOTING GUIDE:
----------------------

Issue: Nodes not executing
Solution: Check browser console for errors

Issue: Workflow stuck at a node
Solution: Check server logs for exceptions

Issue: Results not displaying
Solution: Verify execution completed successfully

Issue: File not found errors
Solution: Check dataset file paths and permissions

Issue: Memory errors
Solution: Reduce dataset size or increase server memory


================================================================================
                    ROBUST, SCALABLE, PRODUCTION-READY!
================================================================================
